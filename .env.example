# VMware Avi LLM Agent - Environment Configuration Example
# Copy this file to .env and edit the values

# ============================================
# LLM PROVIDER CONFIGURATION
# ============================================
# Choose either "ollama" or "mistral"
LLM_PROVIDER=ollama

# ============================================
# MISTRAL AI CONFIGURATION (used when LLM_PROVIDER=mistral)
# ============================================
# Your Mistral AI API Key (required for Mistral)
MISTRAL_API_KEY=your-mistral-api-key-here
MISTRAL_API_BASE_URL=https://api.mistral.ai
MISTRAL_DEFAULT_MODEL=mistral-tiny
MISTRAL_MODELS=mistral-tiny,mistral-small,mistral-medium,mistral-large
MISTRAL_TIMEOUT=60
MISTRAL_TEMPERATURE=0.7
MISTRAL_MAX_TOKENS=2048

# ============================================
# OLLAMA CONFIGURATION (used when LLM_PROVIDER=ollama)
# ============================================
# Ollama service URL
OLLAMA_HOST=http://localhost:11434
OLLAMA_DEFAULT_MODEL=llama3.2
OLLAMA_MODELS=llama3.2,mistral,codellama,llama3.1
OLLAMA_TIMEOUT=60
OLLAMA_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=2048

# ============================================
# VMWARE AVI LOAD BALANCER CONFIGURATION
# ============================================
# Avi Controller connection details
AVI_HOST=your-avi-controller.example.com
AVI_USERNAME=admin
AVI_PASSWORD=your-secure-password
AVI_VERSION=31.2.1
AVI_TENANT=admin
AVI_TIMEOUT=30
AVI_INSECURE=false  # Set to true only for testing with self-signed certificates
AVI_AUTH_METHOD=session  # "session" or "basic" - authentication method

# ============================================
# APPLICATION CONFIGURATION
# ============================================
# Server settings
LOG_LEVEL=info
LOG_FORMAT=json
SERVER_PORT=8080
SERVER_READ_TIMEOUT=30
SERVER_WRITE_TIMEOUT=30
SERVER_IDLE_TIMEOUT=60

# ============================================
# USAGE INSTRUCTIONS
# ============================================
# 1. Copy this file: cp .env.example .env
# 2. Edit the .env file with your actual credentials
# 3. Start with Mistral: docker-compose --env-file .env up -d --scale ollama=0
# 4. Start with Ollama: docker-compose --env-file .env up -d
# 5. Access at: http://localhost:8080

# ⚠️ SECURITY WARNING: Never commit your .env file with sensitive credentials!